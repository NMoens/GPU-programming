---
title: "GPU programming"
author: "[Geert Jan Bex](mailto:geertjan.bex@uhasselt.be)"
institution: "Hasselt University"
format:
  revealjs:
    transiton: slide
    slide-number: true
code-annotations: select
---

## Motivation

GPUs *can* offer significant speedup

::: {.fragment .highlight-red}
But:

- programming GPUs is non-trivial
- not all algorithms are suitable
- code may not be portable
:::

## L'embarras du choix

::: {.fragment .highlight-red}
- CUDA (NVIDIA specific)
- HIP (AMD specific)
:::

- OpenCL (cross-platform)
- OpenMP (with GPU offloading)
- OpenACC (with GPU offloading)
- Kokkos (cross-platform)
- SYCL (cross-platform)

{{< include kokkos/kokkos.qmd >}}


## Nested parallelism

Challenge: how to divide the work?

::: {.incremental .fragment}
- Use `Kokkos::TeamPolicy`
  - League: set of teams
  - Team: set of threads (warp on NVIDIA/wavefront on AMD)
- Use execution patterns for, reduce, scan
- Threads in team synchronize with `Kokkos::TeamThread::fence()`
- Team can share scratch pad memory
- Threads can have private scratch pad
:::


## Nested execution policies

- Top-level: `TeamPolicy`
- Next level
  - `TeamThreadRange`, `TeamThreadMDRange`
  - `TeamVectorRange`, `TeamVectorMDRange`
  - `ThreadVectorRange`, `ThreadVectorMDRange`
  

## Example 1: $\vec{y} = A \cdot \vec{x}$

Where $\vec{y} \in \mathbb{R}^M$, $A \in \mathbb{R}^{M \times N}$, $\vec{x} \in
\mathbb{R}^N$ 

```{.cpp .fragment}
Kokkos::parallel_for("y=A*x", M,
    KOKKOS_LAMBDA(const int i) {
        for (int j = 0; j < N; ++j) {
            y(i) = A(i, j)*x(j);
        }
    });
```

::::: {.columns}
:::: {.column width=70%}
```{.cpp .fragment}
Kokkos::parallel_for("y=A*x",
    Kokkos::TeamPolicy<>(M, Kokkos::AUTO),       // <1>
    KOKKOS_LAMBDA(int i) {
        double row_sum = 0;
        Kokkos::parallel_reduce("y(i)"           // <2>
            Kokkos::TeamThreadRange(team, N),
            [=] (int j, double& lsum) {
                lsum += A(i,j)*x(j);
            }, row_sum);
        y(i) = row_sum;
});
```

1. Create league of teams of threads
2. Exploit inner parallelism
::::
:::: {.column width=30%}
::: {.fragment}
Better performance for $M \ll N$
:::
::::
:::::


## Example 2: $s = \vec{y} \cdot A \cdot \vec{x}$

Where $s \in \mathbb{R}$, $\vec{y} \in \mathbb{R}^M$, $A \in \mathbb{R}^{M
\times N}$, $\vec{x} \in \mathbb{R}^N$ 

```{.cpp .fragmet}
using policy_t = Kokkos::TeamPolicy<>;
float s {0.0f};
Kokkos::parallel_reduce("y*A*x", policy_t(M, Kokkos::AUTO),
    KOKKOS_LAMBDA(const policy_t::member_type& team_member, float& sum) {
        const int i {team_member.league_rank()};
        float row_sum {0.0f};
        Kokkos::parallel_reduce(
            Kokkos::TeamThreadRange(team_member, N),
            [=] (const int j, float& row_sum) {                                 // <1>
                row_sum += A(i, j)*x(j);                                        // <2>
            }, row_sum);
        Kokkos::single(Kokkos::PerTeam(team_member),                            // <3>
            [&]() { result += y(i)*row_sum; });                                 // <4>
    }, s);
```

1. Note: capture by value
2. Note: *A* is accessed row-wise!
3. Only one thread per team should update!
4. Note: capture by reference


## Scratch pads

::::: {.columns}
:::: {.column width=40%}
- Level-0
  - Limited size, fast
  - "Manual L1 cache"
- Level-1
  - Larger, but less performance
  - "Nearest fast storage"
::::
:::: {.column width=60% .fragment}
- Team or thread private memory
  - Per work item temp storage
  - Size $\propto$ nr. threads
- Manually managed cache
  - Cache frequently used data
  - exposes on-core scratch space (e.g., NVIDIA GPU shared memory)
::::
:::::


## How to configure?
